# HPO_RL

## INDEV

Логика работы:
В run_experiment происходит следующее: 
Создается экземпляр вычислительного бекэнда
В идеальном случае работать должно так:
При создании экземпляра бекэнда передается конфиг (модель, подход к вычислению реварда) и данные
При этом в __init__ в self подгружаются данные, потому что они не меняются, все остальное остается в формате конфига
На данный момент во всех методах полная каша, надо привести все к +- одному виду (очевидно они будут отличаться, но все же надо привести к общему виду)
#TODO Для лучшего варианта надо добавить сохранение модели

Затем создается экземпляр среды
В идеальном случае работает так:
Мы передаем гиперпараметры в виде словаря или yaml файла и бекэнд
Среда некоторым образом определяет действия и состояния
Во время step постепенно выбирает гиперпараметры
Если во время step terminated, то передает полученный конфиг гиперпараметров в бэкенд и считает ревард
В средах чуть меньшая каша, но надо проверить все косяки

После этого создается экземпляр агента, в него передается экземпляр среды
Агент обучается, получается оптимальный набор гиперпараметров и модель с ним

ПРОБЛЕМА
Эта постановка просто базово достаточно дурацкая - мы не гиперпараметры оптимизируем, а процесс их подбора :/
Надо проверить насколько эта постановка рабочая в контексте переноса знаний - так что сокращаем размер подбираемых гиперпараметров
Следовательно занимаемся переносом знаний - оптимизируем стратегию выбора оптимизатора, потенциально можем выбирать разные надстройки под оптимизатором, потенциально повторяем статью и делаем автоаугментацию данных, можно придумать что-то еще

Следовательно TODO такой:
Исправить логику - переходим от попыток научиться оптимизировать все гиперпараметры к попыткам выучить модели выбирать общие гиперпараметры (оптимизаторы и их гиперпараметры (lr, планировщики lr, может, другие гиперы))
В целом это должно быть возможно исправить при помощи просто смены передаваемых параметров
table снести, он лишний и шизофреничный
поправить тренеры (чтобы можно было модели передавать списком или одну)
Изменить run_experiment так, чтобы он получал нужные данные, а выдавал обученную модель rl которая умеет выбирать гиперы
Добавить файл tune_model - фактически как раз наша финальная штука, которая должна принимать модель и с выбором rl оптимизатора гиперов и которая будет соответственно обучать модельку с учетом всех данных (тут вопрос как это реализовать - скорее всего в виде few-shot ft и далее обучения итоговой модели)
